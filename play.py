# python play.py è¿è¡Œè®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œä»¿çœŸæ¼”ç¤º

import torch
import numpy as np
import random
from tqdm import tqdm
import matplotlib.pyplot as plt # type: ignore

from agent.agent import Agent
from env.visual import Visualizer
from env.environment import SpaceEnv
from tool import _get_time_path

# éœ€è¦å‡†å¤‡çš„å‚æ•°
NUM_AGENTS = 3  # æ™ºèƒ½ä½“æ•°é‡

TOTAL_TIME = 1800  # 1800s = 30min ä»¿çœŸæ—¶é—´

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# è®¾ç½®éšæœºç§å­
def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)

# å®Œæˆæ‰€æœ‰åˆå§‹åŒ–
def init():
    """å®Œæˆæ‰€æœ‰åˆå§‹åŒ–"""
    print("ğŸ‘Š Start Init......")

    env = SpaceEnv(1, NUM_AGENTS)
    vis = Visualizer(env.map_size, NUM_AGENTS, TOTAL_TIME)

    obs_dim, act_dim, state_dim, num_dim = env.get_dim()
    agent = Agent(obs_dim, act_dim, state_dim, num_dim, DEVICE) # æ™ºèƒ½ä½“
    agent.load("actor", time="latest", epoch="best") # best

    path = _get_time_path("model", "latest")

    print("ğŸ‘Œ Init done")

    return env, vis, agent, path

# ä¸»è®­ç»ƒå¾ªç¯
def play(env, agent):
    """ä¸»è®­ç»ƒå¾ªç¯"""
    set_seed(1)

    # è®°å½•å˜é‡
    h_pos, h_action, h_reward, h_fuel = [], [], [], []
    h_time, h_error = [], []
    # é‡ç½®ç¯å¢ƒ
    env.reset()
    obs = env.get_obss()
    
    ####################################################################################

    # æ”¶é›†ä¸€ä¸ªå›åˆçš„æ•°æ®
    while env.time <= TOTAL_TIME:
        #* 1. ç”ŸæˆåŠ¨ä½œ
        _, actions, _ = agent.get_action(obs) # [1,n,obs] -> [1,n,action]

        #* 2. æ‰§è¡ŒåŠ¨ä½œ
        next_obs, reward, done = env.step(actions) # [N,action] -> [N,obs]

        #* 3. ä¿å­˜ä¿¡æ¯
        h_pos.append(env.pos.squeeze().clone()) # è®°å½•ä½ç½®
        h_action.append(actions[0]) # è®°å½•ç¬¬0ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œ
        for _ in range(env.dec_step-1):
            h_action.append(torch.zeros_like(actions[0]))
        h_reward.append(reward.squeeze()) # è®°å½•æ‰€æœ‰æ™ºèƒ½ä½“çš„æ€»å¥–åŠ±
        h_fuel.append(env.fuel.squeeze().clone()) # è®°å½•ç‡ƒæ–™
        h_time.append(env.time)
        
        #* 5. æ›´æ–°ä¸Šä¸€æ—¶åˆ»çš„æ¶ˆæ¯å’ŒåŠ¨ä½œ
        obs = next_obs

    record = {
        "h_pos": torch.stack(h_pos),
        "h_action": torch.stack(h_action),
        "h_reward": torch.stack(h_reward),
        "h_fuel": torch.stack(h_fuel),
        "target_pos": env.target_pos,
        "h_time": torch.tensor(h_time)
    }

    torch.save(record, "record.pt")
    return record

# ä¸»è®­ç»ƒå¾ªç¯
def show(vis, path):
    """ä¸»è®­ç»ƒå¾ªç¯"""
    record = torch.load("record.pt")

    h_pos = record["h_pos"].detach().cpu().numpy()
    h_action = record["h_action"].detach().cpu().numpy()
    h_reward = record["h_reward"].detach().cpu().numpy()
    h_fuel = record["h_fuel"].detach().cpu().numpy()
    target_pos = record["target_pos"].detach().cpu().numpy()
    h_time = record["h_time"].detach().cpu().numpy()

    # åˆå§‹åŒ–
    agent_scatter = vis.init_scatter(h_pos[0], target_pos)

    # æ¼”ç¤ºå‡½æ•°
    for step in tqdm(range(1, h_pos.shape[0])):
        #* 1. æ£€æŸ¥çª—å£æ˜¯å¦å…³é—­
        if not plt.fignum_exists(vis.fig_3d.number):
            print("çª—å£å·²å…³é—­ï¼Œç¨‹åºè‡ªåŠ¨ç»“æŸã€‚")
            break
        #* 2. æ›´æ–°ä½ç½®ã€åŠ¨ä½œå’Œå¥–åŠ±
        vis.move_scatter(agent_scatter, h_pos[step])
        vis.show_history(h_pos[step], h_pos[step-1])

        plt.draw()
        plt.pause(0.01)  # æ›´æµç•…çš„åŠ¨ç”»

    vis.show_reward(h_reward, h_time)
    vis.show_action(h_action)
    vis.save(path)
    # ä¿¡æ¯æ˜¾ç¤º
    error = np.linalg.norm(h_pos[-1] - target_pos, axis=-1)
    print(f'ğŸ•’ Time:{TOTAL_TIME}   ğŸš€ Spacecraft:{NUM_AGENTS} \n'
          f'ğŸ† Reward:{np.sum(h_reward[-1]):.2f}   ğŸ¯ Error:{np.mean(error):.2f}Â±{np.std(error):.2f} \n')

if __name__ == "__main__":
    env, vis, agent, path = init()

    record = play(env, agent)
    
    show(vis, path)