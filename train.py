# python train_agent.py è®­ç»ƒagentå¼ºåŒ–å­¦ä¹ éƒ¨åˆ†

import time
import torch
import random

from env.environment import SpaceEnv
from agent.agent import Agent
from agent.buffer import ExperienceBuffer
from tool import get_savepath
from other.show import show_agent

#?#################################### å‚æ•° ####################################?#
NUM_AGENT = 3     # æ™ºèƒ½ä½“æ•°é‡
NUM_ENV = 64     # ç¯å¢ƒæ•°é‡
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

MAX_EPOCHS = 100      # æœ€å¤§è®­ç»ƒepoch,æ­£å¼è®­ç»ƒæ”¹ä¸º1000æˆ–è€…3000ã€5000
SAVE_EPOCHS = 50      # ä¿å­˜ç­–ç•¥é—´éš”epoch,æ­£å¼è®­ç»ƒæ”¹ä¸º100æˆ–è€…500
SHOW_EPOCHS = 10       # æ‰“å°å¥–åŠ±é—´éš”epoch

# è®¾ç½®éšæœºç§å­
def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    torch.manual_seed(seed)
    random.seed(seed)

#?#################################### åˆå§‹åŒ– ####################################?#

def init():
    """å®Œæˆæ‰€æœ‰åˆå§‹åŒ–"""
    print("ğŸ‘Š Start Init......")
    env = SpaceEnv(NUM_ENV, NUM_AGENT) # ç¯å¢ƒ

    obs_dim, act_dim, state_dim, num_dim = env.get_dim()
    agent = Agent(obs_dim, act_dim, state_dim, num_dim, DEVICE) # æ™ºèƒ½ä½“

    expe_buffer = ExperienceBuffer(NUM_ENV*env.max_step) # ç»éªŒå›æ”¾æ± 

    print("ğŸ‘Œ Init done")

    return env, agent, expe_buffer

#?#################################### è®­ç»ƒagent ####################################?#

def train_agent(env, agent, expe_buffer):
    """è®­ç»ƒagent"""
    print("â›³ Start Train Actor......")
    set_seed(1)

    # è®°å½•ç”¨äºç›‘æ§è®­ç»ƒ
    actor_losses = []
    critic_losses = []
    reward_history = []
    best_reward = -1e9

    # å‡†å¤‡å·¥ä½œ
    save_path = get_savepath()
    
    obs, done = env.reset()

    init_time = time.time()

    #*#################################### trainå¼€å§‹ ####################################*#
    
    for epoch in range(MAX_EPOCHS+1):
        obs, done = env.reset()
        expe_buffer.clear()

        #*#################################### æ”¶é›†æ•°æ® ####################################*#

        #* 1ã€æ”¶é›†ä¸€æ‰¹æ•°æ®
        while not done:
            # 1ã€ç”ŸæˆåŠ¨ä½œ
            actions, _, log_probs = agent.get_action(obs) # [N,n,obs] -> [N,n,action]

            # 2ã€æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done = env.step(actions) # [N,n,action] -> [N,reward]

            # 3ã€å­˜å‚¨ç»éªŒ
            expe_buffer.push_batch(
                obs,                             # obs:       [N,n,obs]
                actions,                         # act:       [N,n,action]
                log_probs,                       # log_probs: [N]
                torch.sum(reward, dim=-1),       # reward:    [N]
                next_obs,                        # next_obs:  [N,n,next_obs]
            )

            # 4ã€å¤„ç†
            obs = next_obs.detach().clone()

        #*#################################### æ”¶é›†æ•°æ®ç»“æŸ ####################################*#
        mean_reward = torch.mean(reward, dim=0)
        reward_history.append(mean_reward) # è®°å½•æ‰€æœ‰ç¯å¢ƒçš„å¹³å‡å¥–åŠ±

        #* 2ã€ä¿å­˜ç›®å‰æœ€å¥½çš„æ¨¡å‹
        if torch.sum(reward).item() > best_reward and epoch > int(MAX_EPOCHS/2):
            best_reward = torch.sum(reward).item()
            torch.save(agent.actor.state_dict(), f"{save_path}/actor_pth/best_actor.pth")
            torch.save(agent.critic.state_dict(), f"{save_path}/critic_pth/best_critic.pth")

            torch.save(torch.stack(reward_history), f"{save_path}/data/best_reward_history.pt")
            torch.save(actor_losses, f"{save_path}/data/best_actor_losses.pt")
            torch.save(critic_losses, f"{save_path}/data/best_critic_losses.pt")

        #* 3ã€æ›´æ–°actorå’Œcritic
        expe_data = expe_buffer.prepare_all_data()
        actor_loss, critic_loss = agent.update_actor_critic(expe_data)
        actor_losses.append(actor_loss) # è®°å½•
        critic_losses.append(critic_loss) # è®°å½•

        #* 4ã€å®šæœŸæ˜¾ç¤ºè®­ç»ƒç»“æœ
        if epoch % SHOW_EPOCHS == 0:
            use_time_epoch = (time.time()-init_time)/SHOW_EPOCHS
            init_time = time.time()
            print(f"ğŸ•’ Epoch {epoch}|{MAX_EPOCHS}: "
                  f"R:{torch.sum(mean_reward).item():.2f}, A:{actor_loss:.4f}, C:{critic_loss:.2f}, "
                  f"{use_time_epoch:.2f} S/epoch, {use_time_epoch*epoch/60:.2f}|{use_time_epoch*MAX_EPOCHS/60:.2f}min")

        #* 5ã€å®šæœŸä¿å­˜æ¨¡å‹
        if epoch % SAVE_EPOCHS == 0:
            torch.save(agent.actor.state_dict(), f"{save_path}/actor_pth/actor_epoch_{epoch}.pth")
            torch.save(agent.critic.state_dict(), f"{save_path}/critic_pth/critic_epoch_{epoch}.pth")

            torch.save(torch.stack(reward_history), f"{save_path}/data/reward_history.pt")
            torch.save(actor_losses, f"{save_path}/data/actor_losses.pt")
            torch.save(critic_losses, f"{save_path}/data/critic_losses.pt")
            print("Save Done")
            torch.cuda.empty_cache()#æ¸…ç†ç¼“å­˜
    
    #*#################################### trainç»“æŸ ####################################*#
    show_agent(save_path)

    return best_reward

if __name__ == "__main__":
    env, agent, expe_buffer = init()
    #* è®­ç»ƒagent
    best_reward = train_agent(env, agent, expe_buffer)
    #* ç»“æŸ
    print("ğŸ‘ Training completed!, ğŸ¥‡ Best Reward:", best_reward)